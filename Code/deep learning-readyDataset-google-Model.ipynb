{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import imprortant library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\husse\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from keras.utils import to_categorical\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torchvision.models as models\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch import optim , autocast\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torchvision import transforms, datasets\n",
    "from collections import Counter\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.models import googlenet\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.utils.data as data\n",
    "%pylab inline\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wafer_tensors = torch.load('wafer_tensors.pt')\n",
    "label_wm = torch.load('label_wm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wafer_tensors.to(\"cpu\")\n",
    "label_wm.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wafer Tensors Shape: torch.Size([368568, 3, 56, 56])\n",
      "Label Tensors Shape: torch.Size([368568, 1, 36])\n"
     ]
    }
   ],
   "source": [
    "print(\"Wafer Tensors Shape:\", wafer_tensors.shape)\n",
    "print(\"Label Tensors Shape:\", label_wm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "label_wm[0].squeeze()\n",
    "classes = ['C+EL', 'C+EL+L', 'C+EL+S', 'C+ER', 'C+ER+L', 'C+ER+S', 'C+L',\n",
    "       'C+L+EL+S', 'C+L+ER+S', 'C+L+S', 'C+S', 'Center', 'D+EL', 'D+EL+L',\n",
    "       'D+EL+S', 'D+ER', 'D+ER+L', 'D+ER+S', 'D+L', 'D+L+EL+S',\n",
    "       'D+L+ER+S', 'D+L+S', 'D+S', 'Donut', 'EL+L', 'EL+L+S', 'EL+S',\n",
    "       'ER+L', 'ER+S', 'Edge-Loc', 'Edge-Ring', 'L+S', 'Loc', 'Near-full',\n",
    "       'Random', 'Scratch']\n",
    "label_dict = {class_name: i for i, class_name in enumerate(classes)}\n",
    "label = np.argmax(label_wm.squeeze(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35]\n",
      "[10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 12882\n",
      " 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 12000  9990\n",
      " 10000 10000 10000 10000 10000 10378  9680 12045 10779  9685 10392 10737]\n"
     ]
    }
   ],
   "source": [
    "tensor_array = label.numpy()\n",
    "unique_values, value_counts = np.unique(tensor_array, return_counts=True)\n",
    "\n",
    "# Sort values by their counts in descending order\n",
    "sorted_indices = np.argsort(value_counts)[::-1]\n",
    "sorted_values = unique_values[sorted_indices]\n",
    "sorted_counts = value_counts[sorted_indices]\n",
    "\n",
    "# Print or manipulate the sorted values and their counts as needed\n",
    "print(unique_values)\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"BATCH_SIZE\": 512, \"LEARNING_RATE\": 0.0008, \"NUM_EPOCH\": 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        super(BasicDataset, self).__init__()\n",
    "\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(\n",
    "    wafer_tensors, label_wm, test_size=0.2, random_state= 42, shuffle=True\n",
    ")\n",
    "train_X, val_X, train_Y, val_Y = model_selection.train_test_split(\n",
    "    train_X, train_Y, test_size=0.3, random_state=42, shuffle=True\n",
    ")\n",
    "dataset_train = BasicDataset(train_X, train_Y)\n",
    "dataset_val = BasicDataset(val_X, val_Y)\n",
    "dataset_test = BasicDataset(test_X, test_Y)\n",
    "dataset = ConcatDataset([dataset_train, dataset_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.googlenet = models.googlenet(pretrained=True)\n",
    "        num_features = self.googlenet.fc.in_features\n",
    "        self.googlenet.fc = nn.Linear(num_features, num_classes)\n",
    "        print(\"=======\")\n",
    "        print(num_features)\n",
    "    def forward(self, x):\n",
    "        x = self.googlenet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15befbe8710>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "foldperf = {}\n",
    "GoogleNet = GoogLeNet()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(GoogleNet.parameters(), lr=args[\"LEARNING_RATE\"])\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer):\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "        images = images.cpu()\n",
    "        images, labels = images, labels.reshape(\n",
    "            args[\"BATCH_SIZE\"], 36\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        predictions = torch.argmax(output, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    \n",
    "    return train_loss, train_correct\n",
    "\n",
    "\n",
    "def valid_epoch(model, dataloader, loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    for images, labels in dataloader:\n",
    "        images = images.cpu()\n",
    "        images, labels = images, labels.reshape(\n",
    "            args[\"BATCH_SIZE\"], 36\n",
    "        )\n",
    "        output = model(images)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        loss = loss_fn(output, labels)\n",
    "        valid_loss += loss.item() * images.size(0)\n",
    "        predictions = torch.argmax(output, 1)\n",
    "        val_correct += (predictions == labels).sum().item()\n",
    "    return valid_loss, val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/100 AVG Training Loss:0.353 AVG Test Loss:0.268 AVG Training Acc 87.79 % AVG Test Acc 90.07 %\n",
      "Epoch:2/100 AVG Training Loss:0.135 AVG Test Loss:0.167 AVG Training Acc 95.47 % AVG Test Acc 93.39 %\n",
      "Epoch:3/100 AVG Training Loss:0.104 AVG Test Loss:0.162 AVG Training Acc 96.52 % AVG Test Acc 93.78 %\n",
      "Epoch:4/100 AVG Training Loss:0.085 AVG Test Loss:0.134 AVG Training Acc 97.04 % AVG Test Acc 94.77 %\n",
      "Epoch:5/100 AVG Training Loss:0.073 AVG Test Loss:0.131 AVG Training Acc 97.45 % AVG Test Acc 94.82 %\n",
      "Epoch:6/100 AVG Training Loss:0.061 AVG Test Loss:0.131 AVG Training Acc 97.86 % AVG Test Acc 95.09 %\n",
      "Epoch:7/100 AVG Training Loss:0.052 AVG Test Loss:0.134 AVG Training Acc 98.18 % AVG Test Acc 95.06 %\n",
      "Epoch:8/100 AVG Training Loss:0.048 AVG Test Loss:0.130 AVG Training Acc 98.26 % AVG Test Acc 95.20 %\n",
      "Epoch:9/100 AVG Training Loss:0.040 AVG Test Loss:0.133 AVG Training Acc 98.51 % AVG Test Acc 95.25 %\n",
      "Epoch:10/100 AVG Training Loss:0.038 AVG Test Loss:0.136 AVG Training Acc 98.57 % AVG Test Acc 95.37 %\n",
      "Epoch:11/100 AVG Training Loss:0.032 AVG Test Loss:0.123 AVG Training Acc 98.74 % AVG Test Acc 95.67 %\n",
      "Epoch:12/100 AVG Training Loss:0.032 AVG Test Loss:0.162 AVG Training Acc 98.80 % AVG Test Acc 94.86 %\n",
      "Epoch:13/100 AVG Training Loss:0.026 AVG Test Loss:0.148 AVG Training Acc 98.94 % AVG Test Acc 95.20 %\n",
      "Epoch:14/100 AVG Training Loss:0.026 AVG Test Loss:0.143 AVG Training Acc 98.96 % AVG Test Acc 95.53 %\n",
      "Epoch:15/100 AVG Training Loss:0.022 AVG Test Loss:0.154 AVG Training Acc 99.06 % AVG Test Acc 95.39 %\n",
      "Epoch:16/100 AVG Training Loss:0.022 AVG Test Loss:0.156 AVG Training Acc 99.09 % AVG Test Acc 95.44 %\n",
      "Epoch:17/100 AVG Training Loss:0.020 AVG Test Loss:0.160 AVG Training Acc 99.17 % AVG Test Acc 95.40 %\n",
      "Epoch:18/100 AVG Training Loss:0.019 AVG Test Loss:0.152 AVG Training Acc 99.17 % AVG Test Acc 95.65 %\n",
      "Epoch:19/100 AVG Training Loss:0.017 AVG Test Loss:0.146 AVG Training Acc 99.25 % AVG Test Acc 95.89 %\n",
      "Epoch:20/100 AVG Training Loss:0.018 AVG Test Loss:0.152 AVG Training Acc 99.23 % AVG Test Acc 95.78 %\n",
      "Epoch:21/100 AVG Training Loss:0.016 AVG Test Loss:0.166 AVG Training Acc 99.26 % AVG Test Acc 95.50 %\n",
      "Epoch:22/100 AVG Training Loss:0.015 AVG Test Loss:0.155 AVG Training Acc 99.33 % AVG Test Acc 95.70 %\n",
      "Epoch:23/100 AVG Training Loss:0.016 AVG Test Loss:0.177 AVG Training Acc 99.28 % AVG Test Acc 95.13 %\n",
      "Epoch:24/100 AVG Training Loss:0.014 AVG Test Loss:0.146 AVG Training Acc 99.36 % AVG Test Acc 95.89 %\n",
      "Epoch:25/100 AVG Training Loss:0.013 AVG Test Loss:0.160 AVG Training Acc 99.38 % AVG Test Acc 95.55 %\n",
      "Epoch:26/100 AVG Training Loss:0.014 AVG Test Loss:0.144 AVG Training Acc 99.35 % AVG Test Acc 95.83 %\n",
      "Epoch:27/100 AVG Training Loss:0.012 AVG Test Loss:0.157 AVG Training Acc 99.42 % AVG Test Acc 95.57 %\n",
      "Epoch:28/100 AVG Training Loss:0.013 AVG Test Loss:0.151 AVG Training Acc 99.39 % AVG Test Acc 95.99 %\n",
      "Epoch:29/100 AVG Training Loss:0.011 AVG Test Loss:0.179 AVG Training Acc 99.46 % AVG Test Acc 95.58 %\n",
      "Epoch:30/100 AVG Training Loss:0.012 AVG Test Loss:0.157 AVG Training Acc 99.42 % AVG Test Acc 95.74 %\n",
      "Validation loss has not improved for 5 epochs. Stopping training...\n",
      "Average CPU Usage: 35.17%\n",
      "Maximum Memory Usage: 19469.69 MB\n",
      "Fold 2\n",
      "Epoch:1/100 AVG Training Loss:0.035 AVG Test Loss:0.017 AVG Training Acc 98.85 % AVG Test Acc 98.47 %\n",
      "Epoch:2/100 AVG Training Loss:0.019 AVG Test Loss:0.015 AVG Training Acc 99.24 % AVG Test Acc 98.49 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_EPOCH\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     24\u001b[0m     train_loss, train_correct \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[0;32m     25\u001b[0m         GoogleNet, train_loader, criterion, optimizer\n\u001b[0;32m     26\u001b[0m     )\n\u001b[1;32m---> 27\u001b[0m     test_loss, test_correct \u001b[38;5;241m=\u001b[39m valid_epoch(GoogleNet, test_loader, criterion)\n\u001b[0;32m     28\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39msampler)\n\u001b[0;32m     29\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m train_correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39msampler) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[1;32mIn[22], line 32\u001b[0m, in \u001b[0;36mvalid_epoch\u001b[1;34m(model, dataloader, loss_fn)\u001b[0m\n\u001b[0;32m     28\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     29\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images, labels\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m     30\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m36\u001b[39m\n\u001b[0;32m     31\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m output \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     33\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, labels)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m, in \u001b[0;36mGoogLeNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgooglenet(x)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\googlenet.py:174\u001b[0m, in \u001b[0;36mGoogLeNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GoogLeNetOutputs:\n\u001b[0;32m    173\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_input(x)\n\u001b[1;32m--> 174\u001b[0m     x, aux1, aux2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(x)\n\u001b[0;32m    175\u001b[0m     aux_defined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_logits\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\googlenet.py:138\u001b[0m, in \u001b[0;36mGoogLeNet._forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minception4b(x)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# N x 512 x 14 x 14\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minception4c(x)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# N x 512 x 14 x 14\u001b[39;00m\n\u001b[0;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minception4d(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\googlenet.py:227\u001b[0m, in \u001b[0;36mInception.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 227\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(x)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\googlenet.py:219\u001b[0m, in \u001b[0;36mInception._forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tensor]:\n\u001b[0;32m    218\u001b[0m     branch1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch1(x)\n\u001b[1;32m--> 219\u001b[0m     branch2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch2(x)\n\u001b[0;32m    220\u001b[0m     branch3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch3(x)\n\u001b[0;32m    221\u001b[0m     branch4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch4(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\googlenet.py:273\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 273\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[0;32m    274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(x, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "    \n",
    "    print(\"Fold {}\".format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=args[\"BATCH_SIZE\"], sampler=train_sampler, drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset, batch_size=args[\"BATCH_SIZE\"], sampler=test_sampler, drop_last=True\n",
    "    )\n",
    "    history = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": []}\n",
    "    # Initialize variables for early stopping\n",
    "    best_loss = np.inf\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "    # Start CPU and memory profiling\n",
    "    cpu_percent = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    for epoch in range(args[\"NUM_EPOCH\"]):\n",
    "        train_loss, train_correct = train_epoch(\n",
    "            GoogleNet, train_loader, criterion, optimizer\n",
    "        )\n",
    "        test_loss, test_correct = valid_epoch(GoogleNet, test_loader, criterion)\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "\n",
    "        if epoch >= 15:\n",
    "\n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "    \n",
    "            if counter >= patience:\n",
    "                print(\n",
    "                    \"Validation loss has not improved for {} epochs. Stopping training...\".format(\n",
    "                        patience\n",
    "                    )\n",
    "                )\n",
    "                break\n",
    "\n",
    "        print(\n",
    "            \"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(\n",
    "                epoch + 1, args[\"NUM_EPOCH\"], train_loss, test_loss, train_acc, test_acc\n",
    "            )\n",
    "        )\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "        cpu_percent.append(psutil.cpu_percent())\n",
    "        memory_usage.append(psutil.virtual_memory().used)\n",
    "\n",
    "    # Stop CPU and memory profiling\n",
    "    avg_cpu_percent = sum(cpu_percent) / len(cpu_percent)\n",
    "    max_memory_usage = max(memory_usage)\n",
    "    print(\"Average CPU Usage: {:.2f}%\".format(avg_cpu_percent))\n",
    "    print(\"Maximum Memory Usage: {:.2f} MB\".format(max_memory_usage / (1024 ** 2)))\n",
    "    foldperf[\"fold{}\".format(fold + 1)] = history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldperf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testl_f, tl_f, testa_f, ta_f = [], [], [], []\n",
    "k = 6\n",
    "for f in range(1, k + 1):\n",
    "    tl_f.append(np.mean(foldperf[\"fold{}\".format(f)][\"train_loss\"]))\n",
    "\n",
    "    testl_f.append(np.mean(foldperf[\"fold{}\".format(f)][\"test_loss\"]))\n",
    "\n",
    "    ta_f.append(np.mean(foldperf[\"fold{}\".format(f)][\"train_acc\"]))\n",
    "    testa_f.append(np.mean(foldperf[\"fold{}\".format(f)][\"test_acc\"]))\n",
    "    print(\n",
    "    \"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(\n",
    "        np.mean(foldperf[\"fold{}\".format(f)][\"train_loss\"]), np.mean(foldperf[\"fold{}\".format(f)][\"test_loss\"]),np.mean(foldperf[\"fold{}\".format(f)][\"train_acc\"]), np.mean(foldperf[\"fold{}\".format(f)][\"test_acc\"])\n",
    "    ))\n",
    "\n",
    "print(\"Performance of {} fold cross validation\".format(k))\n",
    "print(\n",
    "    \"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(\n",
    "        np.mean(tl_f), np.mean(testl_f), np.mean(ta_f), np.mean(testa_f)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of folds\n",
    "num_folds = 6  # You can change this to match the actual number of folds\n",
    "\n",
    "# Create a grid of subplots for each metric (accuracy and loss)\n",
    "num_rows = 2\n",
    "num_cols = (num_folds + 1) // 2  # Calculate the number of columns needed based on the number of folds\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "fig.suptitle(\"Accuracy and Loss per Epoch for Folds 1 to 5\")\n",
    "\n",
    "\n",
    "\n",
    "for fold_number in range(1, num_folds + 1):\n",
    "    history = foldperf[f\"fold{fold_number}\"]\n",
    "\n",
    "    # Calculate the subplot position\n",
    "    row = (fold_number - 1) // num_cols  # Row number (0 or 1)\n",
    "    col = (fold_number - 1) % num_cols  # Column number (0 or 1)\n",
    "\n",
    "    # Plot training and test loss\n",
    "    axes[row, col].plot(history[\"train_loss\"], label=\"Training Loss\")\n",
    "    axes[row, col].plot(history[\"test_loss\"], label=\"Validation Loss\")\n",
    "    axes[row, col].set_xlabel(\"Epoch\")\n",
    "    axes[row, col].set_ylabel(\"Loss\")\n",
    "    axes[row, col].set_title(f\"Fold {fold_number} Loss\")\n",
    "    axes[row, col].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of folds\n",
    "num_folds = 6  # You can change this to match the actual number of folds\n",
    "\n",
    "# Create a grid of subplots for each metric (accuracy and loss)\n",
    "num_rows = 2\n",
    "num_cols = (num_folds + 1) // 2  # Calculate the number of columns needed based on the number of folds\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "fig.suptitle(\"Accuracy and Loss per Epoch for Folds 1 to 5\")\n",
    "\n",
    "\n",
    "\n",
    "for fold_number in range(1, num_folds + 1):\n",
    "    history = foldperf[f\"fold{fold_number}\"]\n",
    "\n",
    "    # Calculate the subplot position\n",
    "    row = (fold_number - 1) // num_cols  # Row number (0 or 1)\n",
    "    col = (fold_number - 1) % num_cols  # Column number (0 or 1)\n",
    "\n",
    "    # Plot training and test loss\n",
    "    axes[row, col].plot(history[\"train_acc\"], label=\"Training Accuracy\")\n",
    "    axes[row, col].plot(history[\"test_acc\"], label=\"Validation Accuracy\")\n",
    "    axes[row, col].set_xlabel(\"Epoch\")\n",
    "    axes[row, col].set_ylabel(\"accuracy\")\n",
    "    axes[row, col].set_title(f\"Fold {fold_number} accuracy\")\n",
    "    axes[row, col].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(GoogleNet.state_dict(), 'googlenet_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_X, 'train_X.pt')\n",
    "torch.save(train_Y, 'train_Y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(val_X, 'val_X.pt')\n",
    "torch.save(val_Y, 'val_Y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_X, 'test_X.pt')\n",
    "torch.save(test_Y, 'test_Y.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
